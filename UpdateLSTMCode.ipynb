{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "14wO51pbZw8Qp-J0ZqrUlEZAU1afnGSY0",
      "authorship_tag": "ABX9TyPdQU0e6+um295gIPD9aauL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmededrisahmed/fmcs39130/blob/main/UpdateLSTMCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First we will start with the imports:**"
      ],
      "metadata": {
        "id": "4Y0It5G_M5l2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import io\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory"
      ],
      "metadata": {
        "id": "xGrvycFwMbVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files \n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "EFyBIjJ0SEep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data = pd.read_csv('amazon_alexa.tsv')"
      ],
      "metadata": {
        "id": "Y35rvgLFLrkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now use the below code to read the csv in pandas' dataframe\n",
        "data = pd.read_csv(io.StringIO(uploaded['amazon_alexa.tsv'].decode('utf-8')), sep='\\t')\n",
        "data = data[data.verified_reviews != \"Neutral\"]\n",
        "data = data[['verified_reviews', 'feedback']]\n",
        "# if(data['feedback'] == 1).any():\n",
        "#   print(data['verified_reviews'])"
      ],
      "metadata": {
        "id": "Yzqh_iDFqQa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanData(fiald, feedback):\n",
        "    data[fiald] = data[fiald].apply(lambda x: x.lower())\n",
        "    data[fiald] = data[fiald].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
        "\n",
        "    print(data[ data[feedback] == 1 ].size)\n",
        "    print(data[ data[feedback] == 0 ].size)\n",
        "\n",
        "    for idx,row in data.iterrows():\n",
        "      row[0] = row[0].replace('rt',' ')\n",
        "      \n",
        "cleanData('verified_reviews', 'feedback')\n"
      ],
      "metadata": {
        "id": "jpNf7RK1rFAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(data['verified_reviews'].values)\n",
        "X = tokenizer.texts_to_sequences(data['verified_reviews'].values)\n",
        "X = pad_sequences(X)\n",
        "print(X)"
      ],
      "metadata": {
        "id": "tY_5FHI8T8GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = pd.get_dummies(data['feedback']).values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
        "# print(X_train.shape,Y_train.shape)\n",
        "# print(X_test.shape,Y_test.shape)\n",
        "print(Y_test)\n",
        "print(X_test)"
      ],
      "metadata": {
        "id": "abv5zGDTj0MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "max_fatures = 2000\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "gonTesIRLIHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "history = model.fit(X_train, Y_train, epochs = 1, batch_size=batch_size, verbose = 2)"
      ],
      "metadata": {
        "id": "k2Ii3vwd0OzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size = 32\n",
        "# model.fit(X_train, Y_train, epochs = 1, batch_size=batch_size, verbose = 2)"
      ],
      "metadata": {
        "id": "DarjIkFY-OBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the history.history dict to a pandas DataFrame:     \n",
        "hist_df = pd.DataFrame(history.history) \n",
        "hist_df"
      ],
      "metadata": {
        "id": "fwmOJ64D0K2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "UIdA-W2RPFjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred = model.predict(X_test)\n",
        "# # print(y_pred)\n",
        "# s = []\n",
        "# for i in range(y_pred):\n",
        "#   if(y_pred[i] >= 0.5).all():\n",
        "#     s.append(1)\n",
        "#   else:\n",
        "#     s.append(0)\n",
        "\n",
        "# print(s)"
      ],
      "metadata": {
        "id": "fCdsCrSzPxZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files \n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "7n4o5zm-2S2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now use the below code to read the csv in pandas' dataframe\n",
        "df_text = pd.read_csv('twiiter_api_hash_tag.csv')\n",
        "df_text.head()"
      ],
      "metadata": {
        "id": "eWXLer3n2mKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_text.value_counts()"
      ],
      "metadata": {
        "id": "48ZwBSAw55Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_text['Tweet'] = df_text['Tweet'].apply(lambda x: x.lower())\n",
        "df_text['Tweet'] = df_text['Tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
        "# for idx,row in df_text.iterrows():\n",
        "#   row[0] = row[0].replace('rt','')"
      ],
      "metadata": {
        "id": "CeY-UWOn4wV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_text['Tweet']"
      ],
      "metadata": {
        "id": "iVbgmVrzAe74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(df_text['Tweet'].values)\n",
        "test_data = tokenizer.texts_to_sequences(df_text['Tweet'].values)\n",
        "print(test_data)"
      ],
      "metadata": {
        "id": "fnkipXwFg3v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history = model.fit(test_data, epochs = 1, batch_size=128, verbose = 2)\n",
        "\n",
        "pad_test = pad_sequences(test_data, padding=\"post\", maxlen=32)\n",
        "pad_test"
      ],
      "metadata": {
        "id": "7hhvu5U1hSOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod_pridect = model.predict(pad_test)\n",
        "mod_pridect"
      ],
      "metadata": {
        "id": "fy4hg2Z9jf58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl = []\n",
        "for i in range(len(mod_pridect)):\n",
        "  for j in mod_pridect[i]:\n",
        "    if (j >= 0.5).all():\n",
        "      pl.append(1)\n",
        "    else:\n",
        "      pl.append(0)\n",
        "\n",
        "# print(pl)\n",
        "\n",
        "s = []\n",
        "for i in range(len(pl)):\n",
        "  if pl[i] >=0.5:\n",
        "    s.append('positive')\n",
        "  else:\n",
        "    s.append('negative')\n",
        "print(s)"
      ],
      "metadata": {
        "id": "Wa1mTXqskD6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sp = PorterStemmer()\n",
        "# data_new = df_text['Tweet']\n",
        "# data_array = []\n",
        "# data_new\n",
        "# for i in range(len(data_new)):\n",
        "#   data_array.append(data_new)\n",
        "\n",
        "# print(data_array)"
      ],
      "metadata": {
        "id": "Md-ME5ctY0ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t  = ['love', 'love', 'love', 'love', 'love', 'love']\n",
        "tk = tokenizer.texts_to_sequences(t)\n",
        "# print(tk)\n",
        "MAX_LEN = len(tk)\n",
        "pss = pad_sequences(tk, padding = \"post\", maxlen = 32)\n",
        "# print(pss)\n",
        "p = model.predict(pss)\n",
        "print(p)\n",
        "pl = []\n",
        "for i in p:\n",
        "  if (i >= 0.5).all():\n",
        "    pl.append(1)\n",
        "  else:\n",
        "    pl.append(0)\n",
        "for i in range(len(t)):\n",
        "  if pl == 1:\n",
        "    s = 'positive'\n",
        "  else:\n",
        "    s = 'negative'\n",
        "print(s)\n"
      ],
      "metadata": {
        "id": "IzpajgOoSQSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -q transformers"
      ],
      "metadata": {
        "id": "JQXiIT4oCW8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline"
      ],
      "metadata": {
        "id": "Do4MSHLkF-Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # f = df_text['Tweet'][0]\n",
        "# data_new = df_text['Tweet']\n",
        "# data_array = []\n",
        "# data_new\n",
        "# for i in range(len(data_new)):\n",
        "#   data_array.append(data_new[i])\n",
        "# # print(data_array)\n",
        "\n",
        "# sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "# # data = ['love', 'bad']\n",
        "# sentiment_pipeline(data_array)"
      ],
      "metadata": {
        "id": "06AaWKBHGCBQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}