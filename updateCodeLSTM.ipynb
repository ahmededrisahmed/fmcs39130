{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "14wO51pbZw8Qp-J0ZqrUlEZAU1afnGSY0",
      "authorship_tag": "ABX9TyOVBvGEJT/8BQDGVDGr2TCG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmededrisahmed/fmcs39130/blob/main/updateCodeLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First we will start with the imports:**"
      ],
      "metadata": {
        "id": "4Y0It5G_M5l2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import io\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory"
      ],
      "metadata": {
        "id": "xGrvycFwMbVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files \n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "EFyBIjJ0SEep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data = pd.read_csv('amazon_alexa.tsv')"
      ],
      "metadata": {
        "id": "Y35rvgLFLrkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now use the below code to read the csv in pandas' dataframe\n",
        "data = pd.read_csv(io.StringIO(uploaded['amazon_alexa.tsv'].decode('utf-8')), sep='\\t')\n",
        "data = data[data.verified_reviews != \"Neutral\"]\n",
        "data = data[['verified_reviews', 'feedback']]\n",
        "# if(data['feedback'] == 1).any():\n",
        "#   print(data['verified_reviews'])"
      ],
      "metadata": {
        "id": "Yzqh_iDFqQa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanData(fiald, feedback):\n",
        "    data[fiald] = data[fiald].apply(lambda x: x.lower())\n",
        "    data[fiald] = data[fiald].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
        "\n",
        "    print(data[ data[feedback] == 1 ].size)\n",
        "    print(data[ data[feedback] == 0 ].size)\n",
        "\n",
        "    for idx,row in data.iterrows():\n",
        "      row[0] = row[0].replace('rt',' ')\n",
        "      \n",
        "cleanData('verified_reviews', 'feedback')\n"
      ],
      "metadata": {
        "id": "jpNf7RK1rFAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(data['verified_reviews'].values)\n",
        "X = tokenizer.texts_to_sequences(data['verified_reviews'].values)\n",
        "X = pad_sequences(X)\n",
        "print(X)"
      ],
      "metadata": {
        "id": "tY_5FHI8T8GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = pd.get_dummies(data['feedback']).values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
        "# print(X_train.shape,Y_train.shape)\n",
        "# print(X_test.shape,Y_test.shape)\n",
        "print(Y_test)\n",
        "print(X_test)"
      ],
      "metadata": {
        "id": "abv5zGDTj0MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "max_fatures = 2000\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "gonTesIRLIHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "history = model.fit(X_train, Y_train, epochs = 1, batch_size=batch_size, verbose = 2)"
      ],
      "metadata": {
        "id": "k2Ii3vwd0OzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size = 32\n",
        "# model.fit(X_train, Y_train, epochs = 1, batch_size=batch_size, verbose = 2)"
      ],
      "metadata": {
        "id": "DarjIkFY-OBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the history.history dict to a pandas DataFrame:     \n",
        "hist_df = pd.DataFrame(history.history) \n",
        "hist_df"
      ],
      "metadata": {
        "id": "fwmOJ64D0K2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "UIdA-W2RPFjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "print(y_pred)\n",
        "# s = []\n",
        "# for i in range(y_pred):\n",
        "#   if(y_pred[i] >= 0.5).all():\n",
        "#     s.append(1)\n",
        "#   else:\n",
        "#     s.append(0)\n",
        "\n",
        "# print(s)"
      ],
      "metadata": {
        "id": "fCdsCrSzPxZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import modules\n",
        "import pandas as pd\n",
        "import tweepy\n",
        "\n",
        "# function to display data of each tweet\n",
        "def printtweetdata(n, ith_tweet):\n",
        "\t\tprint()\n",
        "\t\tprint(f\"Tweet {n}:\")\n",
        "\t\tprint(f\"Username:{ith_tweet[0]}\")\n",
        "\t\tprint(f\"Description:{ith_tweet[1]}\")\n",
        "\t\tprint(f\"Location:{ith_tweet[2]}\")\n",
        "\t\tprint(f\"Following Count:{ith_tweet[3]}\")\n",
        "\t\tprint(f\"Follower Count:{ith_tweet[4]}\")\n",
        "\t\tprint(f\"Total Tweets:{ith_tweet[5]}\")\n",
        "\t\tprint(f\"Retweet Count:{ith_tweet[6]}\")\n",
        "\t\tprint(f\"Tweet Text:{ith_tweet[7]}\")\n",
        "\t\tprint(f\"Hashtags Used:{ith_tweet[8]}\")\n",
        "\n",
        "\n",
        "# function to perform data extraction\n",
        "def scrape(words, date_since, numtweet):\n",
        "\n",
        "\t\t# Creating DataFrame using pandas\n",
        "\t\tdb = pd.DataFrame(columns=['username',\n",
        "\t\t\t\t\t\t\t\t'description',\n",
        "\t\t\t\t\t\t\t\t'location',\n",
        "\t\t\t\t\t\t\t\t'following',\n",
        "\t\t\t\t\t\t\t\t'followers',\n",
        "\t\t\t\t\t\t\t\t'totaltweets',\n",
        "\t\t\t\t\t\t\t\t'retweetcount',\n",
        "\t\t\t\t\t\t\t\t'text',\n",
        "\t\t\t\t\t\t\t\t'hashtags'])\n",
        "\n",
        "\t\t# We are using .Cursor() to search\n",
        "\t\t# through twitter for the required tweets.\n",
        "\t\t# The number of tweets can be\n",
        "\t\t# restricted using .items(number of tweets)\n",
        "\t\ttweets = tweepy.Cursor(api.user_timeline,\n",
        "\t\t\t\t\t\t\twords, lang=\"en\",\n",
        "\t\t\t\t\t\t\tsince_id=date_since,\n",
        "\t\t\t\t\t\t\ttweet_mode='extended').items(numtweet)\n",
        "\n",
        "\n",
        "\t\t# .Cursor() returns an iterable object. Each item in\n",
        "\t\t# the iterator has various attributes\n",
        "\t\t# that you can access to\n",
        "\t\t# get information about each tweet\n",
        "\t\tlist_tweets = [tweet for tweet in tweets]\n",
        "\n",
        "\t\t# Counter to maintain Tweet Count\n",
        "\t\ti = 1\n",
        "\n",
        "\t\t# we will iterate over each tweet in the\n",
        "\t\t# list for extracting information about each tweet\n",
        "\t\tfor tweet in list_tweets:\n",
        "\t\t\t\tusername = tweet.user.screen_name\n",
        "\t\t\t\tdescription = tweet.user.description\n",
        "\t\t\t\tlocation = tweet.user.location\n",
        "\t\t\t\tfollowing = tweet.user.friends_count\n",
        "\t\t\t\tfollowers = tweet.user.followers_count\n",
        "\t\t\t\ttotaltweets = tweet.user.statuses_count\n",
        "\t\t\t\tretweetcount = tweet.retweet_count\n",
        "\t\t\t\thashtags = tweet.entities['hashtags']\n",
        "\n",
        "\t\t\t\t# Retweets can be distinguished by\n",
        "\t\t\t\t# a retweeted_status attribute,\n",
        "\t\t\t\t# in case it is an invalid reference,\n",
        "\t\t\t\t# except block will be executed\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\t\ttext = tweet.retweeted_status.full_text\n",
        "\t\t\t\texcept AttributeError:\n",
        "\t\t\t\t\t\ttext = tweet.full_text\n",
        "\t\t\t\thashtext = list()\n",
        "\t\t\t\tfor j in range(0, len(hashtags)):\n",
        "\t\t\t\t\t\thashtext.append(hashtags[j]['text'])\n",
        "\n",
        "\t\t\t\t# Here we are appending all the\n",
        "\t\t\t\t# extracted information in the DataFrame\n",
        "\t\t\t\tith_tweet = [username, description,\n",
        "\t\t\t\t\t\t\tlocation, following,\n",
        "\t\t\t\t\t\t\tfollowers, totaltweets,\n",
        "\t\t\t\t\t\t\tretweetcount, text, hashtext]\n",
        "\t\t\t\tdb.loc[len(db)] = ith_tweet\n",
        "\n",
        "\t\t\t\t# Function call to print tweet data on screen\n",
        "\t\t\t\tprinttweetdata(i, ith_tweet)\n",
        "\t\t\t\ti = i+1\n",
        "\t\tfilename = 'scraped_tweets.csv'\n",
        "\n",
        "\t\t# we will save our database as a CSV file.\n",
        "\t\tdb.to_csv(filename)\n",
        "  \n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\t\t# Enter your own credentials obtained\n",
        "\t\t# from your developer account\n",
        "\t\tconsumer_key = \"sDq4iViNrUs01nRtozqjlFMXC\"\n",
        "\t\tconsumer_secret = \"AhMHrrq8qYt4aAueIKiwxjLI2SRHZ4vpI84UMXS8IEojpdCsxt\"\n",
        "\t\taccess_key = \"1588248654548279301-RglNooFZz3LgM5BLCmtNLN0mdS1wpz\"\n",
        "\t\taccess_secret = \"uUx5tsnNqgb3gyFqhLaUjlEAE3YCUYl4rLPQVssYkvEUJ\"\n",
        "\n",
        "\n",
        "\t\tauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "\t\tauth.set_access_token(access_key, access_secret)\n",
        "\t\tapi = tweepy.API(auth)\n",
        "\n",
        "\t\t# Enter Hashtag and initial date\n",
        "\t\tprint(\"Enter Twitter HashTag to search for\")\n",
        "\t\twords = input()\n",
        "\t\tprint(\"Enter Date since The Tweets are required in yyyy-mm--dd\")\n",
        "\t\tdate_since = input()\n",
        "\n",
        "\t\t# number of tweets you want to extract in one run\n",
        "\t\tnumtweet = 100\n",
        "\t\tscrape(words, date_since, numtweet)\n",
        "\t\tprint('Scraping has completed!')\n"
      ],
      "metadata": {
        "id": "A6W3ca-bm8zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = pd.read_csv('scraped_tweets.csv')\n",
        "new_data['text']"
      ],
      "metadata": {
        "id": "6sj17U0Pae5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(new_data['text'].values)\n",
        "final = tokenizer.texts_to_sequences(new_data['text'].values)\n",
        "final = pad_sequences(final)\n",
        "print(final)"
      ],
      "metadata": {
        "id": "QR6iH0Gohbze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_review = \"review.pkl\""
      ],
      "metadata": {
        "id": "zMdcDzY502fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "import pickle\n",
        "pickle.dump(model ,open(file_review,\"wb\"))"
      ],
      "metadata": {
        "id": "WLHq2AeAypB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "with open(file_review, \"rb\") as file:\n",
        "  pic_var = pickle.load(file)"
      ],
      "metadata": {
        "id": "WfCB826Qysfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pic_var"
      ],
      "metadata": {
        "id": "W-cAcH7S1skn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make new prediction\n",
        "new_predict = pic_var.predict(final)"
      ],
      "metadata": {
        "id": "3QxjYEED1x0B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}